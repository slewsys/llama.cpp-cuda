#!/usr/bin/env bash
#
# Copyright Â© 2025 Revolution Robotics, Inc.
#
# SPDX-License-Identifier: MIT
#
# @(#) install-llama.cpp-cuda
#
# This script installs llama.cpp with CUDA and Intel cuBLAS support.
#
# NB: To run llama.cpp, add Intel oneAPI library paths to LD_LIBRARY_PATH, e.g.:
#
#         : ${ONEAPI_ROOT:='/opt/intel/oneapi'}
#         source ${ONEAPI_ROOT}/setvars.sh
#         llama-server ...
#
: ${ASDF_CMD="${HOME}/bin/asdf"}
: ${CAT_CMD:='@CAT_CMD@'}
: ${CMAKE_CMD:='@CMAKE_CMD@'}
: ${ED_CMD:='@ED_CMD@'}
: ${GIT_CMD:='@GIT_CMD@'}
: ${GREP_CMD:='@GREP_CMD@'}
: ${MAKE_CMD:='@MAKE_CMD@'}
: ${PATCH_CMD:='@PATCH_CMD@'}
: ${PIP_CMD:="${HOME}/.asdf/shims/pip"}
: ${READLINK_CMD:='@READLINK_CMD@'}
: ${RM_CMD:='@RM_CMD@'}
: ${SUDO_CMD:='@SUDO_CMD@'}
: ${TAIL_CMD:='@TAIL_CMD@'}

: ${INSTALL_PREFIX:='/usr/local'}

# OS-agnstoic readlink for existent files/directories.
resolve-existing ()
{
    if $READLINK_CMD --version 2>&1 | $GREP_CMD -q 'coreutils'; then
        $READLINK_CMD -e "$@"
    else
        $READLINK_CMD -f N "$@"
    fi
}

check-oneapi-env ()
{
    if test ."$ONEAPI_ROOT" = .''; then
        if test ! -d /opt/intel/oneapi; then
            $SUDO_CMD ${script_dir}/scripts/install-intel-oneapi-hpc || return $?
        fi
        source /opt/intel/oneapi/setvars.sh
    fi
}

check-cuda-env ()
{
    if test ! -d /usr/local/cuda/bin; then
        $SUDO_CMD ${script_dir}/scripts/install-nvidia-cuda || return $?
    fi

    if ! $GREP_CMD -q cuda <<<$PATH; then
        export PATH=/usr/local/cuda/bin:${PATH}
    fi

    if ! command -v nvcc >/dev/null; then
        echo 'Error: CUDA installation not found in /usr/local/cuda/' >&2
        return 1
    fi
}

verify-python ()
{
    local version=$1

    pushd "${script_dir}" >/dev/null || return $?

    trap 'popd >/dev/null; exit 1' 0 1 2 15 RETURN

    if ! command -v "$ASDF_CMD" >/dev/null && test -x "${HOME}/bin/asdf"; then
        export PATH=${PATH}:${HOME}/bin
    fi

    if command -v "$ASDF_CMD" >/dev/null; then
        if [[ ! ."$PATH" =~ ^\..*\.asdf/shims ]]; then
            export PATH=${HOME}/.asdf/shims:${PATH}
        fi

        python_available=$(
            $ED_CMD -s <($ASDF_CMD list) <<EOF | $TAIL_CMD -1
g/^python/1,.d
g/^[a-z]/.,$d
,!$GREP_CMD "$version"
p
Q
EOF
                        ) || return $?

        case "$python_available" in
            *'*'${version}*)
                : Nothing to do
                ;;
            *${version}*)
                echo "python${python_available}" >>${script_dir}/.tool-versions
                ;;
            *)
                $ASDF_CMD plugin add python || return $?
                latest=$($ASDF_CMD latest python 3.11)
                $ASDF_CMD install python "$latest" || return $?
                $ASDF_CMD set python "$latest" || return $?
                ;;
        esac
    else
        ${script_dir}/scripts/install-asdf || return $?
        $ASDF_CMD plugin add python || return $?
        latest=$($ASDF_CMD latest python 3.11)
        $ASDF_CMD install python "$latest" || return $?
        $ASDF_CMD set python "$latest" || return $?
        $ASDF_CMD reshim python || return $?

        export PATH=${HOME}/.asdf/shims:${PATH}:${HOME}/bin
    fi

    popd >/dev/null || return $?

    trap - 0 1 2 15 RETURN
}

install-llama.cpp ()
{
    if test ! -d "${script_dir}/llama.cpp/build"; then
        echo "llama.cpp not found. Please build first." >&2
        return 1
    fi

    $SUDO_CMD $MAKE_CMD -C "${script_dir}/llama.cpp/build" install || return $?

    # gguf.py support for MXFP4 not yet released.
    pushd "${script_dir}/llama.cpp/gguf-py" >/dev/null || return $?

    trap 'popd >/dev/null; exit 1' 0 1 2 15 RETURN

    $PIP_CMD install . || return $?
    popd >/dev/null || return $?

    trap - 0 1 2 15 RETURN
}

caveat-utilitor ()
{
    $CAT_CMD >&2 <<'EOF'
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!! NB: To run llama.cpp, add Intel oneAPI library paths to LD_LIBRARY_PATH, e.g.: !!
!!                                                                                !!
!!        : ${ONEAPI_ROOT:='/opt/intel/oneapi'}                                   !!
!!        source ${ONEAPI_ROOT}/setvars.sh                                        !!
!!        llama-server ...                                                        !!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
EOF
}

if test ."$0" = ."${BASH_SOURCE[0]}"; then
    declare script=''
    declare script_dir=''
    declare script_name=''

    script=$(resolve-existing "$0") || exit $?
    script_name=${script##*/}
    script_dir=${script%/*}

    if (( EUID == 0 )); then
        echo "${script_name}: This script must not be run by user root." >&2
        exit 1
    fi

    echo "Installing llama.cpp with CUDA and Intel cuBLAS support" >&2

    declare py_version='3\.11'

    check-oneapi-env || exit $?
    check-cuda-env || exit $?
    verify-python "$py_version" || exit $?
    install-llama.cpp || exit $?
    caveat-utilitor
fi
