#!/usr/bin/env bash
#
# Copyright © 2025 Revolution Robotics, Inc.
#
# SPDX-License-Identifier: MIT
#
# @(#) build-llama.cpp-cuda
#
# This script builds llama.cpp with CUDA and Intel cuBLAS support.
# Both the latest CUDA and Intel OneAPI hpc toolkit are installed as
# needed. In addition, the asdf version manager is installed as needed
# and used to install the latest Python 3.11.x.
#
: ${ASDF_CMD="${HOME}/bin/asdf"}
: ${CAT_CMD:='@CAT_CMD@'}
: ${CMAKE_CMD:='@CMAKE_CMD@'}
: ${ED_CMD:='@ED_CMD@'}
: ${GIT_CMD:='@GIT_CMD@'}
: ${GREP_CMD:='@GREP_CMD@'}
: ${MAKE_CMD:='@MAKE_CMD@'}
: ${PATCH_CMD:='@PATCH_CMD@'}
: ${PIP_CMD:="${HOME}/.asdf/shims/pip"}
: ${READLINK_CMD:='@READLINK_CMD@'}
: ${RM_CMD:='@RM_CMD@'}
: ${SUDO_CMD:='@SUDO_CMD@'}
: ${TAIL_CMD:='@TAIL_CMD@'}

: ${INSTALL_PREFIX:='/usr/local'}

# OS-agnstoic readlink for existent files/directories.
resolve-existing ()
{
    if $READLINK_CMD --version 2>&1 | $GREP_CMD -q 'coreutils'; then
        $READLINK_CMD -e "$@"
    else
        $READLINK_CMD -f N "$@"
    fi
}

check-oneapi-env ()
{
    if test ."$ONEAPI_ROOT" = .''; then
        if test ! -d /opt/intel/oneapi; then
            $SUDO_CMD ${script_dir}/scripts/install-intel-oneapi-hpc || return $?
        fi
        source /opt/intel/oneapi/setvars.sh
    fi
}

check-cuda-env ()
{
    if test ! -d /usr/local/cuda/bin; then
        $SUDO_CMD ${script_dir}/scripts/install-nvidia-cuda || return $?
    fi

    if ! $GREP_CMD -q cuda <<<$PATH; then
        export PATH=/usr/local/cuda/bin:${PATH}
    fi

    if ! command -v nvcc >/dev/null; then
        echo 'Error: CUDA installation not found in /usr/local/cuda/' >&2
        return 1
    fi
}

verify-python ()
{
    local version=$1

    pushd "${script_dir}" >/dev/null || return $?

    trap 'popd >/dev/null; exit 1' 0 1 2 15 RETURN

    if ! command -v "$ASDF_CMD" >/dev/null && test -x "${HOME}/bin/asdf"; then
        export PATH=${PATH}:${HOME}/bin
    fi

    if command -v "$ASDF_CMD" >/dev/null; then
        if [[ ! ."$PATH" =~ ^\..*\.asdf/shims ]]; then
            export PATH=${HOME}/.asdf/shims:${PATH}
        fi

        python_available=$(
            $ED_CMD -s <($ASDF_CMD list) <<EOF | $TAIL_CMD -1
g/^python/1,.d
g/^[a-z]/.,$d
,!$GREP_CMD "$version"
p
Q
EOF
                        ) || return $?

        case "$python_available" in
            *'*'${version}*)
                : Nothing to do
                ;;
            *${version}*)
                echo "python${python_available}" >>${script_dir}/.tool-versions
                ;;
            *)
                $ASDF_CMD plugin add python || return $?
                latest=$(eval $ASDF_CMD latest python "$version")
                $ASDF_CMD install python "$latest" || return $?
                $ASDF_CMD set python "$latest" || return $?
                ;;
        esac
    else
        ${script_dir}/install-asdf || return $?
        $ASDF_CMD plugin add python || return $?
        latest=$(eval $ASDF_CMD latest python "$version")
        $ASDF_CMD install python "$latest" || return $?
        $ASDF_CMD set python "$latest" || return $?
        $ASDF_CMD reshim python || return $?

        export PATH=${HOME}/.asdf/shims:${PATH}:${HOME}/bin
    fi

    popd >/dev/null || return $?

    trap - 0 1 2 15 RETURN
}

fetch-llama.cpp ()
{
    local repo_url='https://github.com/ggml-org/llama.cpp.git'

    if test ! -d llama.cpp; then
        $GIT_CMD clone --single-branch --filter=tree:0 "$repo_url" || return $?
    else
        $GIT_CMD -C llama.cpp pull --rebase
    fi
}

apply-patches ()
{
    local up_to_date=''

    pushd "${script_dir}/llama.cpp" >/dev/null || return $?

    trap 'popd >/dev/null; exit 1' 0 1 2 15 RETURN

    up_to_date=$(
        $GIT_CMD -C "${script_dir}/llama.cpp" log \
                 --pretty=reference --grep 'Update Python requirements.'
           ) || return $?

    if test -z "$up_to_date" -a \
            -f "${script_dir}/llama.cpp/requirements.txt"; then
        $PATCH_CMD -d "${script_dir}/llama.cpp/requirements" \
                   < "${abs_srcdir}/patches/requirements.diff" || return $?
        $PATCH_CMD -d "${script_dir}/llama.cpp/tools/mtmd" \
                   < "${abs_srcdir}/patches/mtmd-requirements.diff" || return $?
        $PATCH_CMD -d "${script_dir}/llama.cpp/tools/server/tests" \
                   < "${abs_srcdir}/patches/server-tests-requirements.diff" || return $?
        $CAT_CMD >&2 <<'EOF'
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!! Enter git credentials for commit: 'Update Python requirements.': !!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
EOF
        $GIT_CMD -C "${script_dir}/llama.cpp" commit -a -m 'Update Python requirements.'

        $PATCH_CMD -p1 -d "${script_dir}/llama.cpp" \
                   < "${abs_srcdir}/patches/abs-warning.diff" || return $?
        $CAT_CMD >&2 <<'EOF'
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!! Enter git credentials for commit: 'Fix compiler warning. abs() → std::labs().': !!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
EOF
        $GIT_CMD -C "${script_dir}/llama.cpp" commit -a -m 'Fix compiler warning. abs() → std::labs().'
    fi

    $PIP_CMD install -U -r "${script_dir}/llama.cpp/requirements.txt" || return $?
    popd >/dev/null || return $?

    trap - 0 1 2 15 RETURN
}

build-llama.cpp ()
{
    pushd "${script_dir}/llama.cpp" >/dev/null || return $?

    trap 'popd >/dev/null; exit 1' 0 1 2 15 RETURN

    $RM_CMD -rf build || return $?
    $CMAKE_CMD -B build \
               -DCMAKE_INSTALL_PREFIX="${INSTALL_PREFIX}" \
               -DCMAKE_INSTALL_RPATH="${INSTALL_PREFIX}/lib64;${ONEAPI_ROOT}/compiler/2025.2/lib;${ONEAPI_ROOT}/mkl/2025.2/lib" \
               -DGGML_CUDA=ON \
               -DLLAMA_CURL=ON \
               -DGGML_BLAS=ON \
               -DGGML_BLAS_VENDOR=Intel10_64lp \
               -DCMAKE_C_COMPILER=icx \
               -DCMAKE_CXX_COMPILER=icpx \
               -DGGML_NATIVE=ON \
               -DCMAKE_CUDA_ARCHITECTURES="86" \
        || return $?

    $MAKE_CMD -C build -j60
    popd >/dev/null || return $?

    trap - 0 1 2 15 RETURN
}

if test ."$0" = ."${BASH_SOURCE[0]}"; then
    declare script=''
    declare script_dir=''
    declare script_name=''

    script=$(resolve-existing "$0") || exit $?
    script_name=${script##*/}
    script_dir=${script%/*}

    if (( EUID == 0 )); then
        echo "${script_name}: This script must not be run by user root." >&2
        exit 1
    fi

    declare abs_srcdir=$1

    echo "Building llama.cpp with CUDA and Intel cuBLAS support" >&2

    declare py_version='3\.11'

    check-oneapi-env || exit $?
    check-cuda-env || exit $?
    verify-python "$py_version" || exit $?
    fetch-llama.cpp || exit $?
    apply-patches || exit $?
    build-llama.cpp || exit $?
fi
